# ğŸš€ Informe TÃ©cnico â€“ Caso #1: Escalabilidad en Kubernetes

> ğŸ“… **Fecha:** Octubre 2025  
> ğŸ‘¨â€ğŸ’» **Autor:** JosÃ© David Escalante  
> ğŸ§© **Curso:** Kubernetes  
> ğŸ–¥ï¸ **Entorno:** Docker Desktop + Kubernetes local  

---

## ğŸ“– Resumen Ejecutivo

Este proyecto documenta el **anÃ¡lisis, diagnÃ³stico e implementaciÃ³n de una soluciÃ³n de escalabilidad** en una aplicaciÃ³n web desplegada en **Kubernetes**.  
Durante los picos de trÃ¡fico, la aplicaciÃ³n mostraba **lentitud y alto consumo de CPU**, debido a que las rÃ©plicas estaban definidas de forma estÃ¡tica (3 pods fijos).

ğŸ” **Problema identificado:**
> ConfiguraciÃ³n estÃ¡tica de rÃ©plicas sin escalado automÃ¡tico basado en mÃ©tricas.

ğŸ’¡ **SoluciÃ³n implementada:**
> ConfiguraciÃ³n de **Horizontal Pod Autoscaler (HPA)** con **requests/limits** definidos y umbral del **70 % de CPU**.

âœ… **Resultado final:**
- Escalado automÃ¡tico funcional (de 3 a 10 rÃ©plicas)  
- CPU promedio por pod: de **200 m â†’ 120 m**  
- Latencia reducida y mayor estabilidad  

---

## âš™ï¸ TecnologÃ­as Utilizadas

| Componente | DescripciÃ³n |
|-------------|-------------|
| ğŸ³ **Docker Desktop** | Entorno local con Kubernetes habilitado |
| â˜¸ï¸ **Kubernetes v1.28+** | Orquestador principal |
| ğŸŒ **NGINX** | AplicaciÃ³n web de prueba |
| ğŸ“Š **Metrics Server** | Fuente de mÃ©tricas para el HPA |
| âš–ï¸ **HPA (Horizontal Pod Autoscaler)** | Mecanismo de escalado automÃ¡tico |

---

## ğŸ§  DescripciÃ³n del Problema

**SÃ­ntomas observados:**
- Tiempos de respuesta lentos bajo carga.
- Uso de CPU > 200 m en cada pod.
- No existÃ­a escalado automÃ¡tico.
- Solo 3 rÃ©plicas estÃ¡ticas definidas.

**Causa raÃ­z:**
> Ausencia de `resources.requests` / `resources.limits` y de un `HorizontalPodAutoscaler`.

---

## ğŸ”¬ DiagnÃ³stico y AnÃ¡lisis

| MÃ©trica | Estado Inicial | Estado Esperado |
|----------|----------------|-----------------|
| Pods activos | 3 fijos | DinÃ¡mico segÃºn demanda |
| CPU promedio | 190â€“250 m | < 150 m |
| HPA configurado | âŒ No | âœ… SÃ­ |
| Escalado automÃ¡tico | âŒ No | âœ… SÃ­ |

ğŸ“ˆ El clÃºster no reaccionaba al aumento de carga porque **Kubernetes no podÃ­a calcular el uso porcentual de CPU** sin recursos definidos.

---

## ğŸ› ï¸ ImplementaciÃ³n de la SoluciÃ³n

### 1ï¸âƒ£ DefiniciÃ³n de Recursos
```yaml
resources:
  requests:
    cpu: 50m
    memory: 30Mi
  limits:
    cpu: 100m
    memory: 64Mi
```

### 2ï¸âƒ£ ConfiguraciÃ³n del HPA
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-web-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-web-solucion
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

### 3ï¸âƒ£ Despliegue en Kubernetes
```bash
kubectl apply -f deployment-solucion.yaml
kubectl apply -f service-solucion.yaml
kubectl apply -f hpa.yaml
```

---

## ğŸ“Š Resultados y ValidaciÃ³n

| Indicador | Sin HPA | Con HPA | Mejora |
|------------|---------|---------|---------|
| Pods bajo carga | 3 (fijo) | 5â€“7 (dinÃ¡mico) | ğŸ”¼ +133 % |
| CPU promedio/pod | 200 m | 110 m | ğŸ”½ -56 % |
| Latencia | Alta | Estable | âœ… |
| Escalado automÃ¡tico | âŒ | âœ… | âœ… |
| Uso de recursos | Ineficiente | Ã“ptimo | âœ… |

ğŸ“¸ **Evidencia visual:**  
Durante las pruebas de estrÃ©s, el HPA escalÃ³ automÃ¡ticamente de **3 â†’ 7 pods**, distribuyendo la carga de CPU y estabilizando el servicio.

---

## ğŸ§© Arquitectura Final

```text
[Usuarios] 
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service (LoadBalancer)   â”‚
â”‚ app-web-service-solucion â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Deployment: web-solucion â”‚
â”‚ â”œâ”€ Pod1 â”‚ â”œâ”€ Pod2 â”‚ ... â”‚ â”œâ”€ Pod10 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
     Horizontal Pod Autoscaler
     â€¢ Escala entre 3 y 10 pods
     â€¢ Umbral de CPU: 70 %
```

---

## ğŸ§¾ Lecciones Aprendidas

1. **Definir siempre `requests` y `limits`**: son esenciales para el cÃ¡lculo del HPA.  
2. **El CPU es la mÃ©trica ideal** para aplicaciones web ligeras.  
3. **Umbral de 70 %** permite reaccionar antes de la saturaciÃ³n.  
4. **Ventanas de estabilizaciÃ³n (5 min)** evitan oscilaciones constantes (flapping).  
5. **Monitoreo constante** con `kubectl top` y `kubectl get hpa` mejora la observabilidad.

---

## ğŸ§° Archivos Incluidos

- ğŸ“„ `deployment-problema.yaml`  
- ğŸ“„ `deployment-solucion.yaml`  
- âš–ï¸ `hpa.yaml`  
- ğŸŒ `service-problema.yaml` / `service-solucion.yaml`  
- ğŸ§® `metrics-server` instalaciÃ³n  
- ğŸ§¾ `script.ps1` (automatizaciÃ³n)  
- âœ… `checklist-validacion.md`

---

## ğŸ§­ Comandos Ãštiles

```bash
# Ver estado de HPA
kubectl get hpa -n escalabilidad-lab

# Monitorear mÃ©tricas en tiempo real
kubectl top pods -n escalabilidad-lab --containers

# Ver eventos de escalado
kubectl get events -n escalabilidad-lab --sort-by='.lastTimestamp'
```

---

## ğŸ“š Referencias

- [ğŸ“˜ Kubernetes â€“ Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [âš™ï¸ Resource Management for Pods](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
- [ğŸ“Š Metrics Server GitHub](https://github.com/kubernetes-sigs/metrics-server)
- [ğŸ³ Docker Desktop â€“ Enable Kubernetes](https://docs.docker.com/desktop/kubernetes/)

---

## ğŸ ConclusiÃ³n

> El proyecto demuestra la importancia de **diseÃ±ar despliegues dinÃ¡micos** en Kubernetes mediante **HPA y mÃ©tricas de CPU**, logrando asÃ­ **resiliencia, eficiencia y estabilidad** ante variaciones de carga.  

âœ¨ *â€œAutomatizar la respuesta ante la demanda es el primer paso hacia la verdadera elasticidad en la nube.â€*

---

ğŸ“„ **Autor:** JosÃ© David Escalante  
ğŸ“ *Proyecto acadÃ©mico â€“ Caso de Estudio de Escalabilidad Kubernetes (Octubre 2025)*
